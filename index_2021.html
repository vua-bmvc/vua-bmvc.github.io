 <!-- FlatFy Theme - Andrea Galanti /-->
<!doctype html>
<!--[if lt IE 7]> <html class="no-js ie6 oldie" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js ie7 oldie" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js ie8 oldie" lang="en"> <![endif]-->
<!--[if IE 9]>    <html class="no-js ie9" lang="en"> <![endif]-->
<!--[if gt IE 9]><!--> <html> <!--<![endif]-->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0">
    <meta name="description" content="Fourth Multimodal Learning and Applications Workshop ">
    <meta name="author" content="">

    <title>MULA 2021</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
 
    <!-- Custom Google Web Font -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href='http://fonts.googleapis.com/css?family=Lato:100,300,400,700,900,100italic,300italic,400italic,700italic,900italic' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Arvo:400,700' rel='stylesheet' type='text/css'>
	
    <!-- Custom CSS-->
    <link href="css/general.css" rel="stylesheet">
	
	 <!-- Owl-Carousel -->
    <link href="css/custom.css" rel="stylesheet">
	<link href="css/owl.carousel.css" rel="stylesheet">
    <link href="css/owl.theme.css" rel="stylesheet">
	<link href="css/style.css" rel="stylesheet">
	<link href="css/animate.css" rel="stylesheet">
	
	<!-- Magnific Popup core CSS file -->
	<link rel="stylesheet" href="css/magnific-popup.css"> 
	
	<script src="js/modernizr-2.8.3.min.js"></script>  <!-- Modernizr /-->
	<!--[if IE 9]>
		<script src="js/PIE_IE9.js"></script>
	<![endif]-->
	<!--[if lt IE 9]>
		<script src="js/PIE_IE678.js"></script>
	<![endif]-->

	<!--[if lt IE 9]>
		<script src="js/html5shiv.js"></script>
	<![endif]-->

</head>

<body id="home">

	<!-- Preloader -->
	<div id="preloader">
		<div id="status"></div>
	</div>
	
	<!-- FullScreen -->
    <div class="intro-header">
		<div class="col-xs-12 text-center abcen1">
			<h1 class="h1_home wow fadeIn" data-wow-delay="0.4s" style="text-shadow: 0 0 8px #000000;">4<sup>th</sup> Multimodal Learning and Applications Workshop</h1>
			<h3 class="h3_home wow fadeIn" data-wow-delay="0.6s" style="text-shadow: 0px 0px 4px black, 0 0 25px black"> 
				<br></br>
				<br></br>
				<p>In conjunction with <a style="color:white" href="http://cvpr2021.thecvf.com/" target="_blank"><b>CVPR 2021</b></a>. </p> VIRTUAL - June 19<sup>th</sup> 2021 (Morning)</p>
				<!-- <p>Room: Seaside 7</p> -->
			</h3>
			<!--ul class="list-inline intro-social-buttons">
				<li><a href="https://twitter.com/galantiandrea" class="btn  btn-lg mybutton_cyano wow fadeIn" data-wow-delay="0.8s"><span class="network-name">Twitter</span></a>
				</li>
				<li id="download" ><a href="#downloadlink" class="btn  btn-lg mybutton_standard wow swing wow fadeIn" data-wow-delay="1.2s"><span class="network-name">Free Download</span></a>
				</li>
			</ul-->
		</div>    
        <!-- /.container -->
		<div class="col-xs-12 text-center abcen wow fadeIn">
			<div class="button_down "> 
				<a class="imgcircle wow bounceInUp" data-wow-duration="1.5s"  href="#scope"> <img class="img_scroll" src="img/icon/circle.png" alt=""> </a>
			</div>
		</div>
    </div>
	
	<!-- NavBar-->
	<nav class="navbar-default" role="navigation">
		<div class="container">
			<div class="navbar-header">
				<button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
					<span class="sr-only">Toggle navigation</span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
				</button>
				<a class="navbar-brand" href="#home">MULA 2021</a>
			</div>

			<div class="collapse navbar-collapse navbar-right navbar-ex1-collapse">
				<ul class="nav navbar-nav">
					
					<li class="menuItem"><a href="#scope">Home</a></li>
					<li class="menuItem"><a href="#submit">Submit</a></li>
					<li class="menuItem"><a href="#program">Program</a></li>
					<li class="menuItem"><a href="#invited">Invited Speakers</a></li>
					<li class="menuItem"><a href="#organizers">Organizers</a></li>	
					<!-- <li class="menuItem"><a href="#sponsors">Sponsor</a></li>					 -->
					<li class="menuItem"><a href="#contacts">Contacts</a></li>
					<li class="menuItem"><a href="#old_editions">Old Editions</a></li>
				</ul>
			</div>
		   
		</div>
	</nav> 
	
	<!-- Scope -->
    <div id ="scope" class="content-section-a" style="border-top: 0">

        <div class="container">
			
            <div class="row">
			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->
				
                <div class="wow fadeInRightBig" data-animation-delay="200">   
                    <h3 class="section-heading">4<sup>th</sup> Multimodal Learning and Applications Workshop (MULA 2021)</h3>
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
					
		    <p class="lead"  style="text-align:justify">  <b>NEWS! </b>Full recording of the event is available at <u>   <a href="https://www.youtube.com/watch?v=pHuFMcaoLio&ab_channel=MichaelYang" >https://www.youtube.com/watch?v=pHuFMcaoLio&ab_channel=MichaelYang</a>  </u>
		    </p>
                    <p class="lead"  style="text-align:justify">
						The exploitation of the power of big data in the last few years led to a big step forward in many applications of Computer Vision. However, most of the tasks tackled so far are involving visual modality only, mainly due to the unbalanced number of labelled samples available among modalities (e.g., there are many huge labelled datasets for images while not as many for audio or IMU based classification), resulting in a huge gap in performance when algorithms are trained separately.<br/><br/>
						Recently, a few works have started to exploit the synchronization of multimodal streams (e.g., audio/video, RGB/depth, RGB/Lidar, visual/text, text/audio) to transfer semantic information from one modality to another reaching surprising results. Interesting applications are also proposed in a self-supervised fashion, where multiple modalities are learning correspondences without need of manual labelling, resulting in a more powerful set of features compared to those learned processing the two modalities separately. Other works have also shown that particular training paradigms allow neural networks to perform well when one of the modalities is missing due to sensor failure or unfavorable environmental conditions. These topics are gaining lots of interest in computer vision community in the recent years. <br/><br/>
						The information fusion from multiple sensors is a topic of major interest also in industry, the exponential growth of companies working on automotive, drone vision, surveillance or robotics are just a few examples. Many companies are trying to automate processes, by using a large variety of control signals from different sources. The aim of this workshop is to generate momentum around this topic of growing interest, and to encourage interdisciplinary interaction and collaboration between computer vision, multimedia, remote sensing, and robotics communities, that will serve as a forum for research groups from academia and industry. <br/><br/>
						We expect contributions involving, but not limited to, image, video, audio, depth, IR, IMU, laser, text, drawings, synthetic, etc. Position papers with feasibility studies and cross-modality issues with highly applicative flair are also encouraged. Multimodal data analysis is a very important bridge among vision, multimedia, remote sensing, and robotics, therefore we expect a positive response from these communities. <br/><br/>
						
						<b>Potential topics </b> include, but are not limited to: <br/><br/>
						<ul class="lead">
							<li>Multimodal learning</li>
							<li>Cross-modal learning</li>
							<li>Self-supervised learning for multimodal data</li>
							<li>Multimodal data generation and sensors</li>
							<li>Unsupervised learning on multimodal data</li>
							<li>Cross-modal adaptation</li>
							<li>Multimodal data fusion and data representation</li>
							<li>Multimodal transfer learning</li>
							<li>Multimodal scene understanding</li>
							<li>Vision and Language</li>
							<li>Vision and Sound</li>
							<li>Multimodal applications (e.g. drone vision, autonomous driving, industrial inspection, etc.)</li>
						</ul>
					</p>

					

					 <!--p><a class="btn btn-embossed btn-primary" href="#" role="button">View Details</a> 
					 <a class="btn btn-embossed btn-info" href="#" role="button">Visit Website</a></p-->
				</div>   
            </div>
        </div>
        <!-- /.container -->
    </div>

	<!-- Call for Papers -->
    <div id ="submit" class="content-section-b">

        <div class="container">
			
            <div class="row">
			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->
				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading">Submission</h3>
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
                    <p class="lead"  style="text-align:justify">
						Papers will be limited to 8 pages according to the  <u><a href="http://cvpr2021.thecvf.com/node/33#submission-guidelines" Target="blank">CVPR format</a></u> (c.f. main conference authors guidelines). All papers will be reviewed by at least two reviewers with double blind policy. Papers will be selected based on relevance, significance and novelty of results, technical merit, and clarity of presentation. Papers will be published in CVPR 2021 proceedings. 
					</p>
					<p class="lead"  style="text-align:justify">
						All the papers should be submitted using CMT website <u><a href="https://cmt3.research.microsoft.com/MULA2021" Target="blank">https://cmt3.research.microsoft.com/MULA2021</a></u>.
					</p>
				</div>  

				<div class="wow fadeInRightBig" data-animation-delay="200">   
                    <h3 class="section-heading">Important Dates</h3>
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
                    <ul class="lead">
						<del><li>Deadline for submission: March 10<sup>th</sup>, 2021 - 23:59 Pacific Standard Time</li></del>
						<b>---EXTENDED---</b>
						<li>Firm Deadline for submission: March 14<sup>th</sup>, 2021 - 23:59 Pacific Standard Time </li>
						<li>Notification of acceptance  April 8<sup>th</sup>, 2021</li>
						<li>Camera Ready submission deadline: April 19<sup>th</sup>, 2021</li>
						<!-- <b>---EXTENDED---</b> -->
						<!-- <del><li>Camera Ready submission deadline: April 12<sup>th</sup>, 2019</li></del> -->
						<li> Workshop date: June 19<sup>th</sup>, 2021 (Morning)</li>
				</ul>						
				</div>  				
            </div>
        </div>
        <!-- /.container -->
    </div>
	
	<!-- Program -->
    <div id ="program" class="content-section-a">

        <div class="container">				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading">Program</h3>
                    <p class="lead"  style="text-align:justify;font-size:15px">
                    	<!-- Room: Seaside 7 -->
			    <b> N.B.</b>  Time is N. America West Time; [time in brackets is Europe (Central) daylightsaving ]. More details <u> <a href="https://docs.google.com/spreadsheets/d/1AFABD65A_2bYpO07HjbZUkIISKRYH6HkEjaBDIPA0TA/edit#gid=0" >here</a> </u> <br>
			    Zoom and Gatherly links will be available on June 19th
			    
			    <p class="lead"  style="text-align:justify">  Full recording of the event is available at <u>   <a href="https://www.youtube.com/watch?v=pHuFMcaoLio&ab_channel=MichaelYang" >https://www.youtube.com/watch?v=pHuFMcaoLio&ab_channel=MichaelYang</a>  </u>
			    </p>

					<p class="lead"  style="text-align:justify">	
					08:00-08:10 - Welcome from organizers and openings remarks <br>
					[17:00-17:10]
					</p>
					<p class="lead"  style="text-align:justify">
					08:10-08:40 - Keynote - <b> Rogerio Schmidt Feris  </b>  - <i> "Adaptive Multimodal Learning for Efficient Video Understanding" </i>  <br>
					[17:10-17:40]
					
					</p>
					<p class="lead"  style="text-align:justify;font-size:15px;padding-left: 5em">
					  <i> Abstract: </i>  The tremendous growth of multimodal video data in recent years has increased the demand for efficient multimodal deep neural network models, particularly in domains where real-time inference is essential. While significant progress has been made on model compression and acceleration for video understanding, most existing methods rely on one-size-fits-all models, which apply the same amount of computation for all video segments across all modalities. In this talk, I will instead cover methods that adaptively change computation depending on the content of the input. In particular, in the context of audio-visual action recognition, I will describe a method that adaptively decides which modality to use for each video segment (deciding where to look at and listen to in the video), with the goal of improving both accuracy and efficiency. Finally, I will conclude my talk by describing ongoing work that integrates this technology into a system for auto-curation of sports highlights based on multimodal video understanding. <br>
					</p>
					<p class="lead"  style="text-align:justify">
					08:40-09:35 - Oral Session I (5-min presentations)<br>
					[17:40-18:35]
					</p>
					<p class="lead" style="text-align:justify;font-size:15px;padding-left: 5em">
					    (ID 02 - Poster slot 1) - Dealing with Missing Modalities in the Visual Question Answer-Difference Prediction Task through Knowledge Distillation - <i> Jae Won Cho, Dong-Jin Kim, Jinsoo Choi, Yunjae Jung, In So Kweon </i><br>
					    (ID 11 - Poster slot 2) - Beyond VQA: Generating Multi-word Answers and Rationales to Visual Questions - <i>  Radhika Dua, Sai Srinivas Kancheti, Vineeth N Balasubramanian </i><br>
					    (ID 14 - Poster slot 3) - Using Text to Teach Image Retrieval - <i> Haoyu Dong, Ze Wang, Qiang Qiu, Guillermo Sapiro </i><br>
					    (ID 17 - Poster slot 4) - An Improved Attention for Visual Question Answering - <i> Tanzila Rahman, Shih-Han Chou, Leonid Sigal, Giuseppe Carenini</i><br>
					    (ID 19 - Poster slot 5) - Target-Tailored Source-Transformation for Scene Graph Generation - <i>  Wentong Liao, Cuiling Lan, Michael Ying Yang, Wenjun Zeng, Bodo Rosenhahn </i><br>
					    (ID 25 - Poster slot 6) - Private-Shared Disentangled Multimodal VAE for Learning of Latent Representations - <i>  Mihee Lee, Vladimir Pavlovic </i><br>
					    (ID 26 - Poster slot 7) - Editing like Humans: A Contextual, Multimodal Framework for Automated Video Editing - <i> Sharath Koorathota, Patrick J Adelman, Kelly Cotton, Paul Sajda </i><br>
					    (ID 30 - Poster slot 8) - Exploring the Limits of Zero-Shot Learning - How Low Can You Go? - <i> Hemanth Dandu, Karan Sharma, Suchendra M. Bhandarkar </i><br>
					</p>
					    
					<p class="lead"  style="text-align:justify">
					09:35-10:05 - Keynote  - <b> Lorenzo Torresani </b>  - <i> "Vision using Sight...but also Sound and Speech" </i> <a href="https://dartmouth.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=13e99fc6-a1a5-43af-8e28-ad49012c0efd"> (link) </a> <br>
					[18:35-19:05]
					</p>
					<p class="lead"  style="text-align:justify">
					10:05-11:00 - - Oral Session II  (5-min presentations) <br>
					[19:05-20:00]
					</p>
					<p class="lead" style="text-align:justify;font-size:15px;padding-left: 5em">
					    (ID 01 - Poster slot 9) - Self-supervised Feature Learning by Cross-modality and Cross-view Correspondences - <i> Longlong Jing, Ling Zhang, YingLi Tian</i><br>
					    (ID 07 - Poster slot 10) - Adaptive Intermediate Representations for Video Understanding - <i> Juhana Kangaspunta, AJ Piergiovanni, Rico Jonschkowski, Michael S Ryoo, Anelia Angelova</i><br>
					    (ID 10 - Poster slot 11) - Practical Cross-modal Manifold Alignment for Robotic Grounded Language Learning - <i> Andre T Nguyen, Luke Richards, Gaoussou Y Kebe, Edward  Raff, Kasra Darvish, Francis Ferraro, Cynthia Matuszek</i><br>
					    (ID 12 - Poster slot 12) - Progressive Knowledge-Embedded Unified Perceptual Parsing for Scene Understanding - <i> Wenbo Zheng, Lan Yan, Chao Gou, Fei-Yue Wang</i><br>
					    (ID 21 - Poster slot 13) - Radar Camera Fusion via Representation Learning in Autonomous Driving - <i> Xu Dong, Binnan Zhuang, Yunxiang Mao, Langechuan Liu</i><br>
					    (ID 24 - Poster slot 14) - Cross-modal Speaker Verification and Recognition: A Multilingual Perspective - <i> Shah Nawaz, Muhammad Saad Saeed, Pietro Morerio, Arif Mahmood, Ignazio Gallo, Mohammad Haroon Yousaf, Alessio Del Bue</i><br>
					    (ID 31 - Poster slot 15) - APES: Audiovisual Person Search in Untrimmed Video - <i> Juan C Leon, Fabian Caba, Federico Perazzi, Long T Mai, Joon-Young Lee, Bernard Ghanem, Pablo Arbelaez</i><br>
					    (ID 32 - Poster slot 16) - 3D Hand Pose Estimation via aligned latent space injection and kinematic losses  - <i> Andreas Stergioulas, Theocharis Chatzis, Dimitrios Konstantinidis, Kosmas Dimitropoulos, Petros Daras</i><br>
					</p>

					<p class="lead"  style="text-align:justify">
					11:00-11:30 - Keynote - <b>Kostas Daniilidis </b> - <i> "Event- vs frame-based vision" </i> <br> 
					[20:00-20:30]
					</p>
					<p class="lead"  style="text-align:justify">
					11:30-11:35 - Closing Remarks  <br>
					[20:30-20:35]
					</p>
										<p class="lead"  style="text-align:justify">
					11:35-12:00 -  Poster Session (all papers)  <br>
					[20:35-21:00]
					</p>
					
					
					<!-- <p class="lead"  style="text-align:justify">
						11:20 - <b> Spotlight session </b> (3 mins presentation for each poster) <br>
					</p>
					    <p class="lead" style="padding-left: 30em">
						<li><b> Cut Interruption Detection in Laser Cutting Machines: A Machine Learning Approach  </b> - <i>  Giorgio Santolini; Paolo Rota; Paolo Bosetti.</i></li>
						<li><b> A Large-scale Attribute Dataset for Zero-shot Learning </b> - <i> Bo Zhao; Yanwei Fu; Rui Liang; Jiahong Wu; Yonggang Wang; Yizhou Wang.</i></li>
						<li><b> Learning Common Representation from RGB and Depth Images </b> - <i> Giorgio Giannone; Boris Chidlovskii.</i></li>
						<li><b>  Unsupervised Domain Adaptation for Multispectral Pedestrian Detection</b> - <i>  Dayan Guan; Xing Luo; Yanpeng Cao; Jianxin Yang; Yan-long Cao; George Vosselman; Michael Ying Yang.</i></li>
						<li><b> Natural Language Guided Visual Relationship Detection </b> - <i>  Wentong Liao; Bodo Rosenhahn; Lin Shuai; Michael Ying Yang.</i></li>
						<li><b>  Cross-stream Selective Networks for Action Recognition</b> - <i>Bowen Pan; Jiankai Sun; Wuwei Lin; Limin Wang; Weiyao Lin.</i></li>
						
					    </p>
					<p class="lead"  style="text-align:justify">
						12:00 - <b>Poster Session</b> - Pacific Arena Ballroom, Slots 157-166.
					</p>
					<h3 class="section-heading">Presentation Instructions</h3>
					<p>
					<b>Posters:</b> Authors of <b>all</b> accepted papers will present their work in the poster session. Please use  <u><a href="http://cvpr2019.thecvf.com/files/cvpr19_poster_template.pptx" Target="blank">CVPR poster template</a></u>. More instructions can be found in the 'News and Updates' section of <u><a href="http://cvpr2019.thecvf.com" Target="blank">CVPR website</a></u>. Check your Paper ID to find your poster board.
					</p>
					<p>
					<b>Oral talks:</b> Please review the program above to check if you paper was accepted for <b>oral</b> presentation. The presentations will each have a duration of 15 minutes (plus 5 minute for questions). 
					</p>
					<p>
					<b>Spotlights:</b> All other accepted papers will be presented in a 3-minute <b>spotlight</b>, right before the poster session. Please send your slides (pdf) by June 15th at <a href="mailto:mula.workshop@gmail.com">mula.workshop@gmail.com</a>
					</p> -->
				</div>
            </div>
            <div class="container">				
                <div class="wow fadeInRightBig" data-animation-delay="200">
                </div>
            </div>
        </div>
    </div>
	<!-- Invited Speakers -->
    <div id ="invited" class="content-section-b">

        <div class="container">
			
            <div class="row">
				<div class="container">
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<h3 class="section-heading">Invited Speakers</h3>

				    

					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center><a href="https://www.grasp.upenn.edu/people/kostas-daniilidis/" target="blank">Kostas Daniilidis</a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/danilidis.jpg" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">   
						<p> Kostas Daniilidis is Professor of Computer Vision at the Computer and Information Systems Department at the University of Pennsylvania.
						    Kostas’ research interests are in computer vision and robotic perception. His research addresses challenges in the perception of motion and space, such as the geometric design of cameras, and the interplay of geometry and appearance in perception tasks. Kostas’s research gives solutions to perceptual tasks such as panoramic vision, localization, perception of self-motion, large-scale mapping, visual location recognition, 3-D object recognition, and vision-based flocking. Applications of his research involve robot navigation, tele-immersion, and image and shape retrieval.</p>
				</div>
				</div>
				<div class="row wow fadeInRightBig">
					<p>  <wbr> </p>
				</div>
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center><a href="https://www.cs.dartmouth.edu/~lorenzo/home.html" target="blank">Lorenzo Torresani</a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/torresani.jpg" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">   
						<p>Lorenzo Torresani is a Professor in the Computer Science Department at Dartmouth College and a Research Scientist at Facebook AI. He received a Laurea Degree in Computer Science with summa cum laude honors from the University of Milan (Italy) in 1996, and an M.S. and a Ph.D. in Computer Science from Stanford University in 2001 and 2005, respectively. In the past, he has worked at several industrial research labs including Microsoft Research Cambridge, Like.com and Digital Persona. His research interests are in computer vision and deep learning. He is the recipient of several awards, including a CVPR best student paper prize, a National Science Foundation CAREER Award, a Google Faculty Research Award, three Facebook Faculty Awards, and a Fulbright U.S. Scholar Award. </p>
					</div> 
				</div>		
				
				<div class="row wow fadeInRightBig">
					<p> <wbr> </p>
				</div>
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center><a href="https://www.rogerioferis.org/" target="blank">Rogerio Feris</a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/feris.png" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">   
						<p> Rogerio Schmidt Feris is a principal scientist and manager at the MIT-IBM Watson AI lab. He joined IBM in 2006 after receiving a Ph.D. from the University of California, Santa Barbara. He has also worked as an Affiliate Associate Professor at the University of Washington and as an Adjunct Associate Professor at Columbia University. He has authored over 140 technical papers and has over 40 issued patents in the areas of computer vision, multimedia, and machine learning. His current work is particularly focused on deep learning methods that are label-efficient (learning with limited labels), sample-efficient (learning with less data), and computationally efficient. I am also interested in multimodal perception methods that combine vision, sound/speech, and language. </p>
					</div> 
				</div>				
           	 </div>
        	</div>
    	</div>
	</div>
	
	<!-- Organizers -->
    <div id="organizers" class="content-section-a"> 
		
		<div class="container">
		        <div class="row wow fadeInRightBig"  data-animation-delay="200">
				<h3 class="section-heading">Organizers</h3>

				<div class="col-sm-2 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Michael Ying Yang</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/yang.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>University of Twente, Netherlands</i></center></h5>
				</div>
				<div class="col-sm-2 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Pietro Morerio</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/morerio.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Istituto Italiano di Tecnologia, Italy</i></center></h5>
				</div>
				<div class="col-sm-2 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Paolo Rota</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/rota.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Università di Trento, Italy</i></center></h5>
				</div>
				<div class="col-sm-2 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Bodo Rosenhahn</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/rosenhahn.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Institut für Informationsverarbeitung, Leibniz-Universität Hannover, Germany</i></center></h5>
				</div>
				<div class="col-sm-2 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Vittorio Murino</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/murino.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Istituto Italiano di Tecnologia & Università di Verona, Italy & Huawei Technologies, Ireland</i></center></h5>
				</div>
			</div>
        </div>
    </div>

	<!-- Program Committee -->
    <div id ="committee" class="content-section-b" style="border-top: 0">
        <div class="container">			
            <div class="row">			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading"> Acknowledgments</h3>
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
					<p class="lead"  style="text-align:justify">
						We gratefully acknowledge our reviewers
						<ul class="lead">
							Alina Roitberg <br>
							Andrea Pilzer <br>
							Andrea Zunino<br>
							Christoph Reinders<br>
							Dayan Guan<br>
							Giacomo Zara<br>
							Gianluca Scarpellini<br>
							Guanglei Yang<br>
							Haidong Zhu<br>
							Han Zou<br>
							Hanno Ackermann<br>
							Hari Prasanna Das<br>
							Jianfei Yang<br>
							Jiguo Li<br>
							Kohei Uehara<br>
							Kosmas Dimitropoulos<br>
							Letitia E Parcalabescu<br>
							Limin Wang<br>
							Marco Godi<br>
							Mengyi Zhao<br>
							Praneet Dutta<br>
							Ramakrishnan Kannan<br>
							Riccardo Volpi<br>
							Tal Hakim<br>
							Thomas Theodoridis<br>
							Victor G. Turrisi da Costa<br>
							Vladimir Iashin<br>
							Vladimir V Kniaz<br>
							Wenbo Zheng<br>
							Willi Menapace<br>
							Xin Chen<br>
							Yanpeng Cao<br>
						</ul> 
					</p>

					 <!--p><a class="btn btn-embossed btn-primary" href="#" role="button">View Details</a> 
					 <a class="btn btn-embossed btn-info" href="#" role="button">Visit Website</a></p-->
				</div>   
            </div>
        </div>
        <!-- /.container -->
    </div>

    <!-- Sponsor -->
    <!-- <div id ="sponsors" class="content-section-a" style="border-top: 0"> -->
        <!-- <div class="container">			 -->
            <!-- <div class="row">			 -->
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->				
                <!-- <div class="wow fadeInRightBig" data-animation-delay="200">    -->
                    <!-- <h3 class="section-heading"> Sponsor </h3> -->
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
                    <!-- <p class="lead"  style="text-align:justify"> -->
						<!-- <a href=" " target="_blank"><img src="img/sponsor/
X.png" width="300"></a> -->
					<!-- </p> -->
					 <!--p><a class="btn btn-embossed btn-primary" href="#" role="button">View Details</a> 
					 <a class="btn btn-embossed btn-info" href="#" role="button">Visit Website</a></p-->
				<!-- </div>    -->
            <!-- </div> -->
        <!-- </div> -->
        <!-- /.container -->
    <!-- </div> -->
	
    <!-- Old editions -->
    <div id ="old_editions" class="content-section-a" style="border-top: 0"> -->
        <div class="container">						
                <div class="wow fadeInRightBig" data-animation-delay="200">   
					<h3 class="section-heading"> Old Editions </h3>
					
					<ul class="lead">
						<li> <b>1<sup>st</sup> edition @ ECCV 2018 - Munich, Germany </b>, <a href="https://mula2018.github.io/">https://mula2018.github.io</a> </li>
						<li> <b>2<sup>nd</sup> edition @ CVPR 2019 - Long Beach, CA </b>,  <a href="index_2019.html"> 2019 Edition</a></li>
						<li> <b>3<sup>rd</sup> edition @ CVPR 2020 - VIRTUAL </b>,  <a href="https://mul-workshop.github.io/">https://mul-workshop.github.io/</a>  </li>
						
					</ul>
		</div>
    </div>


	<div id="contacts" class="content-section-c ">
			 <!-- Contacts -->
		<div class="container">
			<div class="row">
			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->
				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading" style="color:white">Contacts</h3>
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
                    <p class="lead"  style="text-align:left;color:white">
						For additional info please contact us <u><a style="color:white" href="mailto:mula.workshop@gmail.com">here</a></u> 
					</p>
					
				</div>   
				<!-- <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading"></h3>
                    <p class="lead"  style="text-align:right">
						©MULA2019
					</p>
					
				</div>   	 -->				
            </div>
			<div class="row">
			
						<div class="col-md-6 col-md-offset-3 text-center">
							<div >
									<div class="morph-button ">
										<button type="button"></button>
										
									</div>
							</div>
						</div>	
			</div>
		</div>
	</div>	

   
    <footer>
    
    </footer>

    <!-- JavaScript -->
    <script src="js/jquery-1.10.2.js"></script>
    <script src="js/bootstrap.js"></script>
	<script src="js/owl.carousel.js"></script>
	<script src="js/script.js"></script>
	<!-- StikyMenu -->
	<script src="js/stickUp.min.js"></script>
	<script type="text/javascript">
	  jQuery(function($) {
		$(document).ready( function() {
		  $('.navbar-default').stickUp();
		  
		});
	  });
	
	</script>
	<!-- Smoothscroll -->
	<script type="text/javascript" src="js/jquery.corner.js"></script> 
	<script src="js/wow.min.js"></script>
	<script>
	 new WOW().init();
	</script>
	<script src="js/classie.js"></script>
	<script src="js/uiMorphingButton_inflow.js"></script>
	<!-- Magnific Popup core JS file -->
	<script src="js/jquery.magnific-popup.js"></script> 
</body>

</html>
